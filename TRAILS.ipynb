{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TRAILS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNJIRmQplxU6lsdZGHsaAYj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/romoreira/distributed_learning/blob/main/TRAILS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orW1Ov-AOY39",
        "outputId": "e3155acc-3fd3-4573-b0d6-2193758601cb"
      },
      "source": [
        "!pip install tensorflow\n",
        "!pip install datatime"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.10.0.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.42.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.22.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.13.3)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (12.0.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement datatime (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for datatime\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FB1xEdt5PCw2",
        "outputId": "804d2d8d-25d0-46a0-b15f-8efd7e4489c6"
      },
      "source": [
        "import os\n",
        "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.backend import image_data_format\n",
        "from datetime import datetime\n",
        "\n",
        "import logging\n",
        "import threading\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import copy\n",
        "import random\n",
        "import sys\n",
        "import statistics\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/MyDrive/Colab Notebooks')\n",
        "\n",
        "from build_model import Model\n",
        "import csv\n",
        "\n",
        "# client config\n",
        "NUMOFCLIENTS = 2 # number of client(as particles)\n",
        "SELECT_CLIENTS = 0.5 # c\n",
        "EPOCHS = 1 # number of total iteration\n",
        "CLIENT_EPOCHS = 5 # number of each client's iteration\n",
        "BATCH_SIZE = 10 # Size of batches to train on\n",
        "DROP_RATE = 0\n",
        "\n",
        "# model config \n",
        "LOSS = 'categorical_crossentropy' # Loss function\n",
        "NUMOFCLASSES = 10 # Number of classes\n",
        "lr = 0.0025\n",
        "# OPTIMIZER = SGD(lr=0.015, decay=0.01, nesterov=False)\n",
        "OPTIMIZER = SGD(lr=lr, momentum=0.9, decay=lr/(EPOCHS*CLIENT_EPOCHS), nesterov=False) # lr = 0.015, 67 ~ 69%\n",
        "\n",
        "def load_dataset():\n",
        "    # Code for experimenting with CIFAR-10 datasets.\n",
        "    (X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
        "    \n",
        "    # Code for experimenting with MNIST datasets.\n",
        "    # (X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "    # X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
        "    # X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
        "    \n",
        "    X_train = X_train.astype('float32')\n",
        "    X_test = X_test.astype('float32')\n",
        "    X_train = X_train / 255.0\n",
        "    X_test = X_test / 255.0\n",
        "\n",
        "    Y_train = to_categorical(Y_train)\n",
        "    Y_test = to_categorical(Y_test)\n",
        "\n",
        "    return (X_train, Y_train), (X_test, Y_test)\n",
        "\n",
        "def init_model(train_data_shape):\n",
        "    print(\"Data Shape: \"+str(train_data_shape))\n",
        "    model = Model(loss=LOSS, optimizer=OPTIMIZER, classes=NUMOFCLASSES)\n",
        "    fl_model = model.fl_paper_model(train_shape=train_data_shape)\n",
        "\n",
        "    return fl_model\n",
        "\n",
        "\n",
        "\n",
        "#lists of federated_clients\n",
        "federated_clients_as_threads = []\n",
        "fed_clients_current_round = []\n",
        "\n",
        "#Global Training Model Time\n",
        "global_run_time = 1\n",
        "global_avg = 0\n",
        "(x_train, y_train), (x_test, y_test) = load_dataset()\n",
        "server_model = init_model(train_data_shape=x_train.shape[1:])\n",
        "server_weights = []\n",
        "\n",
        "\n",
        "\n",
        "def utf8len(s):\n",
        "    return len(s.encode('utf-8'))\n",
        "\n",
        "def write_csv(method_name, list):\n",
        "    file_name = '{name}_CIFAR10_randomDrop_{drop}%_output_C_{c}_LR_{lr}_CLI_{cli}_CLI_EPOCHS_{cli_epoch}_TOTAL_EPOCHS_{epochs}_BATCH_{batch}.csv'\n",
        "    file_name = file_name.format(folder=\"origin_drop\",drop=DROP_RATE, name=method_name, c=SELECT_CLIENTS, lr=lr, cli=NUMOFCLIENTS, cli_epoch=CLIENT_EPOCHS, epochs=EPOCHS, batch=BATCH_SIZE)\n",
        "\n",
        "    save_path = \"/content/gdrive/MyDrive/Colab Notebooks\"\n",
        "    completeName = os.path.join(save_path, file_name)\n",
        "\n",
        "    f = open(completeName, 'w', encoding='utf-8', newline='')\n",
        "    wr = csv.writer(f)\n",
        "    \n",
        "    \n",
        "\n",
        "    for l in list:\n",
        "        wr.writerow(l)\n",
        "    \n",
        "    f.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def client_data_config(x_train, y_train):\n",
        "    client_data = [() for _ in range(NUMOFCLIENTS)] # () for _ in range(NUMOFCLIENTS)\n",
        "    num_of_each_dataset = int(x_train.shape[0] / NUMOFCLIENTS)\n",
        "\n",
        "    print(\"Size of x_train: \"+str(len(x_train)))\n",
        "\n",
        "    print(\"Num_of_each_dataset: \"+str(num_of_each_dataset))\n",
        "    \n",
        "    for i in range(NUMOFCLIENTS):\n",
        "        split_data_index = []\n",
        "        while len(split_data_index) < num_of_each_dataset:\n",
        "            item = random.choice(range(x_train.shape[0]))\n",
        "            if item not in split_data_index:\n",
        "                split_data_index.append(item)\n",
        "        \n",
        "        new_x_train = np.asarray([x_train[k] for k in split_data_index])\n",
        "        new_y_train = np.asarray([y_train[k] for k in split_data_index])\n",
        "    \n",
        "        client_data[i] = (new_x_train, new_y_train)\n",
        "\n",
        "    return client_data\n",
        "\n",
        "\n",
        "def fedAVG(server_weight):\n",
        "    #print(\"Server_weight[0]): \"+str(server_weight[0]))\n",
        "    avg_weight = np.array(server_weight[0])\n",
        "    print(\"len(Server_weight[0]): \"+str(len(server_weight)))\n",
        "\n",
        "    if len(server_weight) > 1:\n",
        "        for i in range(1, len(server_weight)):\n",
        "            #print(\"Each i of server_weight: \"+str(server_weight[i]))\n",
        "            avg_weight += server_weight[i]\n",
        "    \n",
        "    avg_weight = avg_weight / len(server_weight)\n",
        "\n",
        "    return avg_weight\n",
        "\n",
        "\n",
        "def client_update(index, client, avg_weight, x_test, y_test):\n",
        "    print(\"Fed_Client Thread {}/{} fitting\\n\".format(index + 1, int(NUMOFCLIENTS * SELECT_CLIENTS)))\n",
        "\n",
        "    if fed_clients_current_round[index] != 0:\n",
        "      print(\"Client: \"+str(index)+ \" is Sleeping for: \"+str(fed_clients_current_round[index]))\n",
        "      time.sleep(fed_clients_current_round[index])\n",
        "    \n",
        "\n",
        "    federated_clients_as_threads[index][2] = federated_clients_as_threads[index][2] + 1#Incrementa o Round que o FED_Client já está treinando\n",
        "\n",
        "    \n",
        "\n",
        "    time_register(index)\n",
        "    client.fit(client_data[index][0], client_data[index][1],\n",
        "          epochs=EPOCHS,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          verbose=0,\n",
        "          validation_split=0.2,\n",
        "    )\n",
        "    time_register(index)\n",
        "\n",
        "    print(\"\\n\\n\\nEnd of training Thread: \"+str(index))\n",
        "    print(\"\\n\\n\\n Validating the client training: \"+str(index))\n",
        "    scores = fed_client_evaluation(client, x_test, y_test)\n",
        "    print(\"\\n\\nThe accuracy of Client \"+str(index)+\" is: \"+str(scores)+\" \\n\\n\\n\\n\")\n",
        "    send_model_to_server(scores, index, client)\n",
        "\n",
        "def fed_client_evaluation(model, x_test, y_test):\n",
        "    return model.evaluate(x_test, y_test, batch_size=BATCH_SIZE, verbose=1)\n",
        "\n",
        "\n",
        "def send_model_to_server(metric_scores, client_index, fed_model_trained):\n",
        "    print(\"\\n\\n\\nEnviando metricas para o server. Cliente: \"+str(client_index))\n",
        "    global server_weights\n",
        "    federated_clients_as_threads[client_index].append(metric_scores)\n",
        "    federated_clients_as_threads[client_index].append(fed_model_trained)\n",
        "    server_weights.append(copy.deepcopy(fed_model_trained.get_weights()))\n",
        "\n",
        "    \n",
        "\n",
        "def waiter():\n",
        "    all_fed_finished = False\n",
        "    run_time_training_acc = []\n",
        "    global global_run_time\n",
        "    global fed_clients_current_round\n",
        "    global global_avg\n",
        "    global server_model\n",
        "    global server_weights\n",
        "    while(True):\n",
        "        if global_run_time == 1:\n",
        "          print(\"\\n Current status of Global Training: \"+str(federated_clients_as_threads))\n",
        "          time.sleep(10)\n",
        "          for i in range(len(federated_clients_as_threads)):\n",
        "              if federated_clients_as_threads[i][0].is_alive() == True:\n",
        "                  all_fed_finished = False\n",
        "                  break\n",
        "              else:\n",
        "                all_fed_finished = True\n",
        "          if all_fed_finished:\n",
        "              print(\"Todos os federados do Round terminaram a rodada de treinamento. Calcular os tempos e por pra rodar denovo\")\n",
        "              avg = []\n",
        "              for i in range (len(federated_clients_as_threads)):\n",
        "                  avg.append(federated_clients_as_threads[i][4][1])\n",
        "              print(\"AVG: \"+str(statistics.mean(avg)))\n",
        "              global_avg = statistics.mean(avg)\n",
        "              print(\"STD: \"+str(statistics.stdev(avg)))\n",
        "              \n",
        "              avg_weight = fedAVG(server_weights)\n",
        "              server_model.set_weights(avg_weight)\n",
        "              server_weights = []\n",
        "              \n",
        "              global_run_time = global_run_time + 1\n",
        "              for i in range(len(federated_clients_as_threads)):\n",
        "                print(\"Status do fed na interação 1: \"+str(federated_clients_as_threads[i][0]))\n",
        "                fed_clients_current_round.append(0)#Reseta para que apos a inicializacao todos os feds tenham oportundiade de treinar uma segunda vez\n",
        "                #federated_clients_as_threads[i][4] = []#Reseta a lista de metricas na segunda interação\n",
        "                federated_clients_as_threads[i][0].start()\n",
        "        else:\n",
        "          for i in range(len(federated_clients_as_threads)):\n",
        "            if not(federated_clients_as_threads[i][0].is_alive()):\n",
        "              federated_clients_as_threads[i][0].stat()\n",
        "          \n",
        "          while(True):\n",
        "            print(\"\\n Current status of Global Training: \"+str(federated_clients_as_threads))\n",
        "            time.sleep(10)\n",
        "            for i in range(len(federated_clients_as_threads)):\n",
        "                if federated_clients_as_threads[i][0].is_alive() == True:\n",
        "                    all_fed_finished = False\n",
        "                    break\n",
        "                else:\n",
        "                  all_fed_finished = True\n",
        "            if all_fed_finished:\n",
        "              print(\"Todos os federados do Round terminaram a rodada de treinamento ou estao dormindo penalizados. Calcular os tempos e por pra rodar denovo\")\n",
        "              acc_client = []\n",
        "              run_time_average = []\n",
        "              for i in range (len(federated_clients_as_threads)):\n",
        "                  acc_client.append(federated_clients_as_threads[i][4][1])\n",
        "                  run_time_average.append(federated_clients_as_threads[i][3][1]-federated_clients_as_threads[i][3][1])\n",
        "                  print(\"Tempo de treinamento medio global: \"+str(statistics.mean(run_time_average)))\n",
        "              \n",
        "              for i in range (len(federated_clients_as_threads)):\n",
        "                  if federated_clients_as_threads[i][4][1] >= global_avg:\n",
        "                    federated_clients_as_threads[i][2] = EPOCHS * 2\n",
        "                    fed_clients_current_round[i] = 0\n",
        "                  else:\n",
        "                    federated_clients_as_threads[i][2] = EPOCHS+1\n",
        "                    fed_clients_current_round[i] = statistics.mean(run_time_average)\n",
        "\n",
        "              \n",
        "              print(\"AVG: \"+str(statistics.mean(acc_client)))\n",
        "              global_avg = statistics.mean(acc_client)\n",
        "              print(\"STD: \"+str(statistics.stdev(acc_client)))\n",
        "              \n",
        "              avg_weight = fedAVG(server_weights)\n",
        "              server_model.set_weights(avg_weight)\n",
        "              server_weights = []\n",
        "              \n",
        "              global_run_time = global_run_time + 1\n",
        "              \n",
        "\n",
        "          \n",
        "\n",
        "def fed_client_selection(policy):\n",
        "\n",
        "    if policy == \"TRAILS\":\n",
        "        print(\"Decidir qual cliente escolher\")\n",
        "        #https://github.com/dnanhkhoa/simple-bloom-filter\n",
        "\n",
        "def time_register(index_fed_client):\n",
        "      now = datetime.now()\n",
        "      #current_time = now.strftime(\"%H:%M:%S\")\n",
        "      federated_clients_as_threads[index_fed_client][3].append(now)   \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    \n",
        "    federated_clients_as_threads = []\n",
        "\n",
        "    \n",
        "    server_model.summary()\n",
        "\n",
        "    client_data = client_data_config(x_train, y_train)\n",
        "    print(\"Client_data: \"+str(len(client_data)))\n",
        "    fl_models = []\n",
        "    for i in range(NUMOFCLIENTS):\n",
        "        fl_models.append(init_model(train_data_shape=client_data[i][0].shape[1:]))\n",
        "\n",
        "\n",
        "    avg_weight = np.zeros_like(server_model.get_weights())\n",
        "    print(\"AVG_Weight: \"+str(avg_weight))\n",
        "    server_evaluate_acc = []\n",
        "\n",
        "    print(\"NUMOFCLIENTS: \"+str(NUMOFCLIENTS))\n",
        "    print(\"Select_clients: \"+str(SELECT_CLIENTS))\n",
        "\n",
        "\n",
        "    waiter = threading.Thread(target=waiter, args=(), daemon=True)\n",
        "    waiter.start()\n",
        "    \n",
        "      \n",
        "    for index, client in enumerate(fl_models):\n",
        "   \n",
        "      #print(\"Index: \"+str(index))\n",
        "      #print(\"CLIENT: \"+str(client))\n",
        "      fed = []\n",
        "      a = threading.Thread(target=client_update, args=(index, client, avg_weight, x_test, y_test,))\n",
        "      fed.append(a)\n",
        "      fed.append(index)\n",
        "      fed.append(0)#Round que o cliente já está treinando\n",
        "\n",
        "      federated_clients_as_threads.append(fed)\n",
        "\n",
        "    for i in range(len(federated_clients_as_threads)):\n",
        "        fed_clients_current_round.append(0)\n",
        "        federated_clients_as_threads[i].append([])\n",
        "        federated_clients_as_threads[i][0].start()\n",
        "\n",
        "    waiter.join()\n",
        "    print(federated_clients_as_threads)\n",
        "    for i in range(len(federated_clients_as_threads)):\n",
        "        federated_clients_as_threads[i][0].join()\n",
        "    \n",
        "\n",
        "#    write_csv(\"FedAvg\", server_evaluate_acc)\n",
        "      \n",
        "    \n",
        "  "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Shape: (32, 32, 3)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 32, 32, 32)        2432      \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 32)        25632     \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 16, 16, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16, 16, 32)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 16, 16, 64)        51264     \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 16, 16, 64)        102464    \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 8, 8, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 4096)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               2097664   \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,284,586\n",
            "Trainable params: 2,284,586\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Size of x_train: 50000\n",
            "Num_of_each_dataset: 25000\n",
            "Client_data: 2\n",
            "Data Shape: (32, 32, 3)\n",
            "Data Shape: (32, 32, 3)\n",
            "AVG_Weight: [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "NUMOFCLIENTS: 2\n",
            "Select_clients: 0.5\n",
            "\n",
            " Current status of Global Training: []\n",
            "Fed_Client Thread 1/1 fitting\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fed_Client Thread 2/1 fitting\n",
            "\n",
            "\n",
            " Current status of Global Training: [[<Thread(Thread-12, started 139883280377600)>, 0, 2, [datetime.datetime(2021, 12, 3, 20, 1, 35, 685181)]], [<Thread(Thread-13, started 139883271984896)>, 1, 2, [datetime.datetime(2021, 12, 3, 20, 1, 35, 735881)]]]\n",
            "\n",
            " Current status of Global Training: [[<Thread(Thread-12, started 139883280377600)>, 0, 2, [datetime.datetime(2021, 12, 3, 20, 1, 35, 685181)]], [<Thread(Thread-13, started 139883271984896)>, 1, 2, [datetime.datetime(2021, 12, 3, 20, 1, 35, 735881)]]]\n",
            "\n",
            " Current status of Global Training: [[<Thread(Thread-12, started 139883280377600)>, 0, 2, [datetime.datetime(2021, 12, 3, 20, 1, 35, 685181)]], [<Thread(Thread-13, started 139883271984896)>, 1, 2, [datetime.datetime(2021, 12, 3, 20, 1, 35, 735881)]]]\n",
            "\n",
            " Current status of Global Training: [[<Thread(Thread-12, started 139883280377600)>, 0, 2, [datetime.datetime(2021, 12, 3, 20, 1, 35, 685181)]], [<Thread(Thread-13, started 139883271984896)>, 1, 2, [datetime.datetime(2021, 12, 3, 20, 1, 35, 735881)]]]\n",
            "\n",
            " Current status of Global Training: [[<Thread(Thread-12, started 139883280377600)>, 0, 2, [datetime.datetime(2021, 12, 3, 20, 1, 35, 685181)]], [<Thread(Thread-13, started 139883271984896)>, 1, 2, [datetime.datetime(2021, 12, 3, 20, 1, 35, 735881)]]]\n",
            "\n",
            " Current status of Global Training: [[<Thread(Thread-12, started 139883280377600)>, 0, 2, [datetime.datetime(2021, 12, 3, 20, 1, 35, 685181)]], [<Thread(Thread-13, started 139883271984896)>, 1, 2, [datetime.datetime(2021, 12, 3, 20, 1, 35, 735881)]]]\n",
            "\n",
            " Current status of Global Training: [[<Thread(Thread-12, started 139883280377600)>, 0, 2, [datetime.datetime(2021, 12, 3, 20, 1, 35, 685181)]], [<Thread(Thread-13, started 139883271984896)>, 1, 2, [datetime.datetime(2021, 12, 3, 20, 1, 35, 735881)]]]\n",
            "\n",
            " Current status of Global Training: [[<Thread(Thread-12, started 139883280377600)>, 0, 2, [datetime.datetime(2021, 12, 3, 20, 1, 35, 685181)]], [<Thread(Thread-13, started 139883271984896)>, 1, 2, [datetime.datetime(2021, 12, 3, 20, 1, 35, 735881)]]]\n",
            "\n",
            "\n",
            "\n",
            "End of training Thread: 0\n",
            "\n",
            "\n",
            "\n",
            " Validating the client training: 0\n",
            "\n",
            "\n",
            "\n",
            "End of training Thread: 1\n",
            "\n",
            "\n",
            "\n",
            " Validating the client training: 1\n",
            " 370/1000 [==========>...................] - ETA: 8s - loss: 5.4917 - accuracy: 0.2884\n",
            " Current status of Global Training: [[<Thread(Thread-12, started 139883280377600)>, 0, 2, [datetime.datetime(2021, 12, 3, 20, 1, 35, 685181), datetime.datetime(2021, 12, 3, 20, 3, 0, 220926)]], [<Thread(Thread-13, started 139883271984896)>, 1, 2, [datetime.datetime(2021, 12, 3, 20, 1, 35, 735881), datetime.datetime(2021, 12, 3, 20, 3, 0, 223862)]]]\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 5.4973 - accuracy: 0.2873\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 5.3741 - accuracy: 0.3457\n",
            "\n",
            "\n",
            "The accuracy of Client 0 is: [5.374068260192871, 0.3456999957561493] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Enviando metricas para o server. Cliente: 0\n",
            "\n",
            " Current status of Global Training: [[<Thread(Thread-12, stopped 139883280377600)>, 0, 2, [datetime.datetime(2021, 12, 3, 20, 1, 35, 685181), datetime.datetime(2021, 12, 3, 20, 3, 0, 220926)], [5.374068260192871, 0.3456999957561493], <keras.engine.sequential.Sequential object at 0x7f39904b6850>], [<Thread(Thread-13, started 139883271984896)>, 1, 2, [datetime.datetime(2021, 12, 3, 20, 1, 35, 735881), datetime.datetime(2021, 12, 3, 20, 3, 0, 223862)]]]\n",
            "\n",
            "\n",
            "The accuracy of Client 1 is: [5.497265815734863, 0.2872999906539917] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Enviando metricas para o server. Cliente: 1\n",
            "Todos os federados do Round terminaram a rodada de treinamento. Calcular os tempos e por pra rodar denovo\n",
            "AVG: 0.3164999932050705\n",
            "STD: 0.041295039629064605\n",
            "len(Server_weight[0]): 2\n",
            "Status do fed na interação 1: <Thread(Thread-12, stopped 139883280377600)>\n",
            "[[<Thread(Thread-12, stopped 139883280377600)>, 0, 2, [datetime.datetime(2021, 12, 3, 20, 1, 35, 685181), datetime.datetime(2021, 12, 3, 20, 3, 0, 220926)], [5.374068260192871, 0.3456999957561493], <keras.engine.sequential.Sequential object at 0x7f39904b6850>], [<Thread(Thread-13, stopped 139883271984896)>, 1, 2, [datetime.datetime(2021, 12, 3, 20, 1, 35, 735881), datetime.datetime(2021, 12, 3, 20, 3, 0, 223862)], [5.497265815734863, 0.2872999906539917], <keras.engine.sequential.Sequential object at 0x7f39903abbd0>]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:137: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:143: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "Exception in thread Thread-11:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-2-24a0db975e42>\", line 226, in waiter\n",
            "    federated_clients_as_threads[i][0].start()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 848, in start\n",
            "    raise RuntimeError(\"threads can only be started once\")\n",
            "RuntimeError: threads can only be started once\n",
            "\n"
          ]
        }
      ]
    }
  ]
}