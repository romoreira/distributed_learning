{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Adaptative_Client_Selection_FED.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPoY4f4hznPKEWizJpxxkrh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/romoreira/distributed_learning/blob/main/Adaptative_Client_Selection_FED.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orW1Ov-AOY39"
      },
      "source": [
        "!pip install tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n69iTNH1O8sR"
      },
      "source": [
        "import os\n",
        "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.backend import image_data_format\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import copy\n",
        "import random\n",
        "import sys\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/MyDrive/Colab Notebooks')\n",
        "\n",
        "from build_model import Model\n",
        "import csv\n",
        "\n",
        "# client config\n",
        "NUMOFCLIENTS = 5 # number of client(as particles)\n",
        "SELECT_CLIENTS = 0.5 # c\n",
        "EPOCHS = 30 # number of total iteration\n",
        "CLIENT_EPOCHS = 5 # number of each client's iteration\n",
        "BATCH_SIZE = 10 # Size of batches to train on\n",
        "DROP_RATE = 0\n",
        "\n",
        "# model config \n",
        "LOSS = 'categorical_crossentropy' # Loss function\n",
        "NUMOFCLASSES = 10 # Number of classes\n",
        "lr = 0.0025\n",
        "# OPTIMIZER = SGD(lr=0.015, decay=0.01, nesterov=False)\n",
        "OPTIMIZER = SGD(lr=lr, momentum=0.9, decay=lr/(EPOCHS*CLIENT_EPOCHS), nesterov=False) # lr = 0.015, 67 ~ 69%\n",
        "\n",
        "\n",
        "def write_csv(method_name, list):\n",
        "    file_name = '{name}_CIFAR10_randomDrop_{drop}%_output_C_{c}_LR_{lr}_CLI_{cli}_CLI_EPOCHS_{cli_epoch}_TOTAL_EPOCHS_{epochs}_BATCH_{batch}.csv'\n",
        "    file_name = file_name.format(folder=\"origin_drop\",drop=DROP_RATE, name=method_name, c=SELECT_CLIENTS, lr=lr, cli=NUMOFCLIENTS, cli_epoch=CLIENT_EPOCHS, epochs=EPOCHS, batch=BATCH_SIZE)\n",
        "    f = open(file_name, 'w', encoding='utf-8', newline='')\n",
        "    wr = csv.writer(f)\n",
        "    \n",
        "    for l in list:\n",
        "        wr.writerow(l)\n",
        "    f.close()\n",
        "\n",
        "\n",
        "def load_dataset():\n",
        "    # Code for experimenting with CIFAR-10 datasets.\n",
        "    (X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
        "    \n",
        "    # Code for experimenting with MNIST datasets.\n",
        "    # (X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "    # X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
        "    # X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
        "    \n",
        "    X_train = X_train.astype('float32')\n",
        "    X_test = X_test.astype('float32')\n",
        "    X_train = X_train / 255.0\n",
        "    X_test = X_test / 255.0\n",
        "\n",
        "    Y_train = to_categorical(Y_train)\n",
        "    Y_test = to_categorical(Y_test)\n",
        "\n",
        "    return (X_train, Y_train), (X_test, Y_test)\n",
        "\n",
        "\n",
        "def init_model(train_data_shape):\n",
        "    print(\"Data Shape: \"+str(train_data_shape))\n",
        "    model = Model(loss=LOSS, optimizer=OPTIMIZER, classes=NUMOFCLASSES)\n",
        "    fl_model = model.fl_paper_model(train_shape=train_data_shape)\n",
        "\n",
        "    return fl_model\n",
        "\n",
        "\n",
        "def client_data_config(x_train, y_train):\n",
        "    client_data = [() for _ in range(NUMOFCLIENTS)] # () for _ in range(NUMOFCLIENTS)\n",
        "    num_of_each_dataset = int(x_train.shape[0] / NUMOFCLIENTS)\n",
        "    \n",
        "    for i in range(NUMOFCLIENTS):\n",
        "        split_data_index = []\n",
        "        while len(split_data_index) < num_of_each_dataset:\n",
        "            item = random.choice(range(x_train.shape[0]))\n",
        "            if item not in split_data_index:\n",
        "                split_data_index.append(item)\n",
        "        \n",
        "        new_x_train = np.asarray([x_train[k] for k in split_data_index])\n",
        "        new_y_train = np.asarray([y_train[k] for k in split_data_index])\n",
        "    \n",
        "        client_data[i] = (new_x_train, new_y_train)\n",
        "\n",
        "    return client_data\n",
        "\n",
        "\n",
        "def fedAVG(server_weight):\n",
        "    #print(\"Server_weight[0]): \"+str(server_weight[0]))\n",
        "    avg_weight = np.array(server_weight[0])\n",
        "    print(\"len(Server_weight[0]): \"+str(len(server_weight)))\n",
        "\n",
        "    if len(server_weight) > 1:\n",
        "        for i in range(1, len(server_weight)):\n",
        "            print(\"Each i of server_weight: \"+str(server_weight[i]))\n",
        "            avg_weight += server_weight[i]\n",
        "    \n",
        "    avg_weight = avg_weight / len(server_weight)\n",
        "\n",
        "    return avg_weight\n",
        "\n",
        "\n",
        "def client_update(index, client, now_epoch, avg_weight):\n",
        "    print(\"client {}/{} fitting\".format(index + 1, int(NUMOFCLIENTS * SELECT_CLIENTS)))\n",
        "\n",
        "    if now_epoch != 0:\n",
        "        client.set_weights(avg_weight) \n",
        "    \n",
        "    client.fit(client_data[index][0], client_data[index][1],\n",
        "        epochs=CLIENT_EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        verbose=1,\n",
        "        validation_split=0.2,\n",
        "    )\n",
        "\n",
        "    return client\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0mgqwGFO_zO"
      },
      "source": [
        "def utf8len(s):\n",
        "    return len(s.encode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FB1xEdt5PCw2"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    (x_train, y_train), (x_test, y_test) = load_dataset()\n",
        "\n",
        "    #print(\"X_Train: \"+str(x_train))\n",
        "    #print(\"Y_train: \"+str(y_train))\n",
        "\n",
        "    server_model = init_model(train_data_shape=x_train.shape[1:])\n",
        "    server_model.summary()\n",
        "\n",
        "    client_data = client_data_config(x_train, y_train)\n",
        "    print(\"Client_data: \"+str(len(client_data)))\n",
        "    fl_model = []\n",
        "    for i in range(NUMOFCLIENTS):\n",
        "        fl_model.append(init_model(train_data_shape=client_data[i][0].shape[1:]))\n",
        "\n",
        "    #print(\"FL_model: \"+str(fl_model))\n",
        "\n",
        "    #print(\"server_model.get_weights: \"+str(server_model.get_weights()))\n",
        "    avg_weight = np.zeros_like(server_model.get_weights())\n",
        "    print(\"AVG_Weight: \"+str(avg_weight))\n",
        "    server_evaluate_acc = []\n",
        "\n",
        "    print(\"NUMOFCLIENTS: \"+str(NUMOFCLIENTS))\n",
        "    print(\"Select_clients: \"+str(SELECT_CLIENTS))\n",
        "\n",
        "    for epoch in range(EPOCHS):  \n",
        "        server_weight = []\n",
        "        \n",
        "        selected_num = int(max(NUMOFCLIENTS * SELECT_CLIENTS, 1))\n",
        "        print(\"Selected_num: \"+str(selected_num))\n",
        "        split_data_index = []\n",
        "        while len(split_data_index) < selected_num:\n",
        "            item = random.choice(range(len(fl_model)))\n",
        "            print(\"Item: \"+str(item))\n",
        "            if item not in split_data_index:\n",
        "                split_data_index.append(item)\n",
        "        split_data_index.sort()\n",
        "        \n",
        "        print(\"slplit_data_index.sort(): \"+str(split_data_index))\n",
        "\n",
        "        selected_model = [fl_model[k] for k in split_data_index]\n",
        "\n",
        "        print(\"Selected_model: \"+str(len(selected_model)))\n",
        "\n",
        "\n",
        "        for index, client in enumerate(selected_model):\n",
        "            \n",
        "            recv_model = client_update(index, client, epoch, avg_weight)\n",
        "            print(\"Tamanho do RECV_MODEL: \"+str(utf8len(str(recv_model))))\n",
        "            \n",
        "            rand = random.randint(0,99)\n",
        "            drop_communication = range(DROP_RATE)\n",
        "            if rand not in drop_communication:\n",
        "                server_weight.append(copy.deepcopy(recv_model.get_weights()))\n",
        "        \n",
        "        avg_weight = fedAVG(server_weight)\n",
        "\n",
        "        #print(\"avg_weight: \"+str(avg_weight))\n",
        "\n",
        "        break;\n",
        "\n",
        "        server_model.set_weights(avg_weight)\n",
        "        print(\"server {}/{} evaluate\".format(epoch + 1, EPOCHS))\n",
        "        server_evaluate_acc.append(server_model.evaluate(x_test, y_test, batch_size=BATCH_SIZE, verbose=1))\n",
        "\n",
        "    write_csv(\"FedAvg\", server_evaluate_acc)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}